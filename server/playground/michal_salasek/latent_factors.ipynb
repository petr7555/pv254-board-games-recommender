{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc669642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e93096",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa4b0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to distinguish whether jupyter notebook is part of gitlab repo,\n",
    "# and whether plots/charts/dataframes should be exported and where to\n",
    "def is_development_env():\n",
    "    dir_names = os.getcwd().split(os.sep)\n",
    "    return not(dir_names[-1] == 'michal_salasek' and dir_names[-2] == 'playground')\n",
    "\n",
    "DEVELOPMENT = is_development_env()\n",
    "\n",
    "data_relative_path = './data/' if DEVELOPMENT else '../../data/'\n",
    "ratings_path = os.path.abspath(data_relative_path + 'user_ratings.csv')\n",
    "\n",
    "matrices_export_path = os.path.abspath('../../app/db')\n",
    "ADD_TIMESTAMP = DEVELOPMENT\n",
    "\n",
    "# column names for convenience\n",
    "ITEM = 'BGGId'\n",
    "GAME = ITEM\n",
    "USER = 'Username'\n",
    "RATING = 'Rating'\n",
    "\n",
    "SHOW = DEVELOPMENT  # print info, charts etc.\n",
    "\n",
    "CUTOFF_THRESHOLD = 10\n",
    "k_latent_factors = 3\n",
    "n_epochs = 30\n",
    "learning_rate = 0.005\n",
    "_lambda = 0.02\n",
    "n_epochs_per_RMSE = 1  # how often should we compute RMSE (once every three epochs etc.)\n",
    "chunk_size = 32 * 2\n",
    "epoch_improvement_threshold = 0.005 # improvement in RMSE of validation set being less than `value` will result in early stopping\n",
    "EARLY_STOP = True # apply early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d6d77",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043bc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each Username and board game, keep only the last rating (if a user has rated a game multiple times)\n",
    "\n",
    "def keep_last_rating(df):\n",
    "    df = df.drop_duplicates(['Username', 'BGGId'], keep='last')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ca4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove users with less than `threshold` ratings\n",
    "# remove games with less than `threshold` ratings\n",
    "def drop_less_than_n(df, col, threshold):\n",
    "    return df[df[col].map(df[col].value_counts()) >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "121dfadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_dataframe(threshold = CUTOFF_THRESHOLD):\n",
    "    users_threshold = threshold\n",
    "    games_threshold = threshold\n",
    "    ratings = pd.read_csv(ratings_path)\n",
    "    ratings = keep_last_rating(ratings)\n",
    "    ratings = drop_less_than_n(ratings, 'Username', users_threshold)\n",
    "    ratings = drop_less_than_n(ratings, 'BGGId', games_threshold)\n",
    "    ratings['Rating'] = ratings['Rating'].astype('float32')\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17ccdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_processed_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd3fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW:\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e8c39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_games = df['BGGId'].unique()\n",
    "unique_users = df['Username'].unique()\n",
    "\n",
    "if SHOW:\n",
    "    rows = len(unique_users)\n",
    "    cols = len(unique_games)\n",
    "    print(f'rows (=users): {rows}')\n",
    "    print(f'cols (=games): {cols}')\n",
    "    print(f'number of cells = {rows} x {cols} = {rows*cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0879dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurrence(df, col):\n",
    "    return df.groupby([col])[col].count().sort_values(ascending=False)\n",
    "\n",
    "def get_user_occurrence(df):\n",
    "    return get_occurrence(df, USER)\n",
    "\n",
    "def get_game_occurrence(df):\n",
    "    return get_occurrence(df, ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ff16a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barchart(series, title, y_label='games', head=True):\n",
    "\n",
    "    # print top 10 labels (usernames / BGGId's)\n",
    "    labels = series.head(10) if head else series.tail(10)\n",
    "    print(labels.index.tolist())\n",
    "    \n",
    "    n_items = len(series)\n",
    "    \n",
    "    x = np.arange(n_items)    \n",
    "    y = series.values\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(title + f'_{n_items}')\n",
    "    plt.xlabel('# of ratings')\n",
    "    plt.ylabel(f'# of {y_label} with given rating')\n",
    "    plt.bar(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef259656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > split all data into training+validation and test set (80/20)\n",
    "# > split the training+validation set into training and validation set (80/20 or 75/25)\n",
    "\n",
    "# we use 'BGGId' column as 'stratify' for train_test_split to get a rating for every game\n",
    "# that way we get proportionally many ratings for game 'X' in train / validation / test set\n",
    "# the same can be done for 'Username'\n",
    "\n",
    "def train_val_test_split(df, stratify_col=None):    \n",
    "    train_val_size = 0.8\n",
    "    test_size = 1 - train_val_size\n",
    "    stratify = df[[stratify_col]] if stratify_col else None\n",
    "    X_train_val, X_test = train_test_split(df, train_size=train_val_size, test_size=test_size, stratify=stratify)\n",
    "\n",
    "    train_size = 0.8\n",
    "    val_size = 1 - train_size\n",
    "    \n",
    "    stratify = X_train_val[[stratify_col]] if stratify_col else None\n",
    "    X_train, X_val = train_test_split(X_train_val, train_size=train_size, test_size=val_size, stratify=stratify)\n",
    "    \n",
    "    return [X_train, X_val, X_test]\n",
    "\n",
    "X_train, X_val, X_test = train_val_test_split(df, GAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "696ae553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a quick verification that data is split properly (as described above)\n",
    "def plot_charts_for_data_split(X_train, X_val, X_test, occurrence=GAME):\n",
    "    \n",
    "    if occurrence == GAME:\n",
    "        X_train_occ = get_game_occurrence(X_train)\n",
    "        X_val_occ = get_game_occurrence(X_val)\n",
    "        X_test_occ = get_game_occurrence(X_test)\n",
    "    else:\n",
    "        X_train_occ = get_user_occurrence(X_train)\n",
    "        X_val_occ = get_user_occurrence(X_val)\n",
    "        X_test_occ = get_user_occurrence(X_test)\n",
    "\n",
    "    plot_barchart(X_train_occ.head(50), f\"X_train_{occurrence}_top\")\n",
    "    plot_barchart(X_val_occ.head(50), f\"X_val_{occurrence}_top\")\n",
    "    plot_barchart(X_test_occ.head(50), f\"X_test_{occurrence}_top\")\n",
    "    \n",
    "    plot_barchart(X_train_occ.tail(50), f\"X_train_{occurrence}_bottom\", head=False)\n",
    "    plot_barchart(X_val_occ.tail(50), f\"X_val_{occurrence}_bottom\", head=False)\n",
    "    plot_barchart(X_test_occ.tail(50), f\"X_test_{occurrence}_bottom\", head=False)\n",
    "\n",
    "if SHOW:\n",
    "    plot_charts_for_data_split(X_train, X_val, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87fe1cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_count(df, col):\n",
    "    return len(df[col].unique())\n",
    "\n",
    "def print_unique_count(X_train, X_val, X_test, col):\n",
    "    X_train_unique = get_unique_count(X_train, col)\n",
    "    X_val_unique = get_unique_count(X_val, col)\n",
    "    X_test_unique = get_unique_count(X_test, col)\n",
    "    \n",
    "    print(f'X_train_unique_{col}: {X_train_unique}')\n",
    "    print(f'X_val_unique_{col}: {X_val_unique}')\n",
    "    print(f'X_test_unique_{col}: {X_test_unique}')\n",
    "\n",
    "if SHOW:\n",
    "    print(f'[ total unique games: {get_unique_count(df, GAME)} ]')\n",
    "    print_unique_count(X_train, X_val, X_test, GAME)\n",
    "    print()\n",
    "    print(f'[ total unique users: {get_unique_count(df, USER)} ]')\n",
    "    print_unique_count(X_train, X_val, X_test, USER)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0225a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW:\n",
    "    print(\"\\n\" + 20 * \"=\" + \"[ X_train ]\" + 20 * \"=\")\n",
    "    print(X_train.info())\n",
    "    print(\"\\n\" + 20 * \"=\" + \"[ X_val ]\" + 20 * \"=\")\n",
    "    print(X_val.info())\n",
    "    print(\"\\n\" + 20 * \"=\" + \"[ X_test ]\" + 20 * \"=\")\n",
    "    print(X_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d7c1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timestamp(name):\n",
    "    return name + \"_\" + strftime(\"%Y-%m-%d_%H-%M-%S\", gmtime())\n",
    "\n",
    "\n",
    "def timer_function(func):\n",
    "    def wrap_func(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        t2 = time.time()\n",
    "        print(f'Function {func.__name__!r} executed in {(t2-t1):.4f}s')\n",
    "        return result\n",
    "    return wrap_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b198c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, name, method='pickle', timestamp=True):\n",
    "    \n",
    "    if timestamp:\n",
    "        name = add_timestamp(name)\n",
    "    \n",
    "    if method == 'h5':\n",
    "        store = pd.HDFStore('store.h5')\n",
    "        store[name] = df\n",
    "    \n",
    "    elif method == 'pickle':\n",
    "        df.to_pickle(f'{name}.pkl')\n",
    "    \n",
    "    \n",
    "def load_df(name, method='pickle'):\n",
    "    \n",
    "    if method == 'h5':\n",
    "        store = pd.HDFStore('store.h5')\n",
    "        return store[name]\n",
    "    \n",
    "    if method == 'pickle':\n",
    "        return pd.read_pickle(f'{name}.pkl')\n",
    "    \n",
    "# save DFs into .pkl files so that they can be loaded for the endpoint\n",
    "def save_dfs(P, Q, add_timestamp=ADD_TIMESTAMP):\n",
    "    save_df(P, 'user_factors', 'pickle', add_timestamp)\n",
    "    save_df(Q, 'item_factors', 'pickle', add_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88562b4f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49ab8eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we wanna consider global effects, add variables for user/item bias and global mean later:\n",
    "# return mean + user_bias + item_bias + np.dot(user_factors, item_factors.T)\n",
    "def rating_prediction(user_factors, item_factors):\n",
    "    return np.dot(user_factors, item_factors.T)\n",
    "\n",
    "# currently not used due to this being applicable to a single user/item_factors vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c6103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(P, Q, ratings):\n",
    "    user_ids = ratings['Username']\n",
    "    item_ids = ratings['BGGId']\n",
    "\n",
    "    user_factors = P.loc[user_ids].values\n",
    "    item_factors = Q.loc[item_ids].values\n",
    "\n",
    "    actual_ratings = ratings['Rating'].values\n",
    "    predicted_ratings = np.einsum('ij, ij->i', user_factors, item_factors)\n",
    "    \n",
    "    error = mean_squared_error(predicted_ratings, actual_ratings)\n",
    "    return sqrt(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e3ecfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rule for early stopping probably could be adjusted based on number of epochs, e.g.:\n",
    "# a) if there are 100 epochs, it might make sense to only evaluate after e.g. every 5 epochs,\n",
    "#    and if the validation error has increased, stop training\n",
    "# b) if there are only < 10 epochs, we might wanna evaluate validation error after every epoch,\n",
    "#    and stop training after validation error has increased once/twice/... in a row\n",
    "def should_early_stop(validation_errors):\n",
    "    if len(validation_errors) < 2:\n",
    "        return False\n",
    "    error_increased = validation_errors[-1] > validation_errors[-2]\n",
    "    improvement_value = validation_errors[-2] - validation_errors[-1]\n",
    "    return error_increased or improvement_value < epoch_improvement_threshold\n",
    "\n",
    "\n",
    "def plot_errors(train_error, val_error, n_epochs):\n",
    "    \n",
    "    if len(train_error) < 2:\n",
    "        return\n",
    "    \n",
    "    x = [i for i in range(len(train_error))]\n",
    "    \n",
    "    # plot lines\n",
    "    plt.plot(x, train_error, label = \"train_error\")\n",
    "    plt.plot(x, val_error, label = \"val_error\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99d9be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_info():\n",
    "    print()\n",
    "    print(20*\"=\" + '[ training summary ]' + 20*\"=\")\n",
    "    print()\n",
    "    print(f'k latent factors: {k_latent_factors}')\n",
    "    print(f'n epochs: {n_epochs}')\n",
    "    print(f'chunk size: {chunk_size}')\n",
    "    print(f'learning rate: {learning_rate}')\n",
    "    print(f'regularization coefficient: {_lambda}')\n",
    "    print(f'RMSE frequence: once every {n_epochs_per_RMSE} epochs ... how often is RMSE calculated')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5abe189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size): \n",
    "    chunks = list()\n",
    "    num_chunks = math.ceil(len(df) / chunk_size)\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df.iloc[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed85d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initialized_matrix(unique_elements, col_prefix, k_latent_factors):\n",
    "    # users_factors matrix of dimensions U x k\n",
    "    matrix_data = 3 * np.random.rand(len(unique_elements), k_latent_factors)\n",
    "    matrix = pd.DataFrame(data = matrix_data, \n",
    "                  index = unique_elements,\n",
    "                  columns = [f'{col_prefix}_{i+1}' for i in range(k_latent_factors)])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07b86424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses minibatch\n",
    "def fit(X_train, X_val, unique_users, unique_items):\n",
    "    \n",
    "    # users_factors matrix of dimensions U x k\n",
    "    P = get_initialized_matrix(unique_users, 'user_feature', k_latent_factors)\n",
    "    \n",
    "    # items_factors matrix of dimensions I x k\n",
    "    Q = get_initialized_matrix(unique_items, 'item_feature', k_latent_factors)\n",
    "\n",
    "    train_error = []\n",
    "    val_error = []\n",
    "    early_stop_flag = False\n",
    "\n",
    "    if SHOW:\n",
    "        print_training_info()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        if SHOW:\n",
    "            print()\n",
    "            print(10*\"=\" + f'[ epoch #{epoch+1} ]' + 10*\"=\")\n",
    "            print()\n",
    "        \n",
    "        epoch_start = time.time()\n",
    "        batch = 0\n",
    "        chunks = split_dataframe(X_train, chunk_size)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            \n",
    "            batch += 1\n",
    "            if SHOW and batch % (len(chunks) // 5) == 1:\n",
    "                print(f'batch #{batch}')\n",
    "            \n",
    "            user_ids = chunk['Username']\n",
    "            item_ids = chunk['BGGId']\n",
    "            \n",
    "            user_factors = P.loc[user_ids].values\n",
    "            item_factors = Q.loc[item_ids].values\n",
    "            \n",
    "            actual_rating = chunk['Rating'].values\n",
    "            # pair-wise multiplication + sum = dot product for user_factors[i], item_factors[i]\n",
    "            predicted_rating = np.einsum('ij, ij->i', user_factors, item_factors)\n",
    "            \n",
    "            error = actual_rating - predicted_rating\n",
    "            \n",
    "            # allow to multiply shape of (chunks, k_latent_factors) /=factors/ with (chunks,) /=error/\n",
    "            # error[0] value multiplies each element of user/item_factors[0] (u1, u2, u3)\n",
    "            P_err = item_factors * error[:, None]\n",
    "            Q_err = user_factors * error[:, None]\n",
    "\n",
    "            P_gradient = learning_rate * (P_err - _lambda * user_factors)\n",
    "            Q_gradient = learning_rate * (Q_err - _lambda * item_factors)\n",
    "            \n",
    "            \n",
    "            P.loc[user_ids] += P_gradient\n",
    "            Q.loc[item_ids] += Q_gradient\n",
    "        \n",
    "        # either every n-th epoch (starting at first) or when it's the last epoch\n",
    "        should_compute_RMSE = (epoch % n_epochs_per_RMSE == 0) or (epoch == n_epochs - 1)\n",
    "        if should_compute_RMSE and EARLY_STOP:\n",
    "            \n",
    "            train_rmse = RMSE(P, Q, X_train)\n",
    "            val_rmse = RMSE(P, Q, X_val)\n",
    "\n",
    "            train_error.append(train_rmse)\n",
    "            val_error.append(val_rmse)\n",
    "        \n",
    "            if SHOW:\n",
    "                print()\n",
    "                print(f'X_train RMSE: {train_rmse}')\n",
    "                print(f'X_val RMSE: {val_rmse}')\n",
    "            \n",
    "            # if the validation error has increased, it might be a sign of overfitting\n",
    "            early_stop_flag = should_early_stop(val_error)\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        if SHOW:\n",
    "            print()\n",
    "            print(f'epoch took {epoch_end - epoch_start} seconds')\n",
    "    \n",
    "        if early_stop_flag:\n",
    "            if SHOW:\n",
    "                print()\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "            break\n",
    "    \n",
    "    if SHOW:\n",
    "        plot_errors(train_error, val_error, epoch+1)\n",
    "        \n",
    "    return [P, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e449997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "P, Q = fit(X_train, X_val, unique_users, unique_games)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "if SHOW:\n",
    "    print(f'{end - start} seconds; X_train size: {len(X_train)}, X_val size: {len(X_val)}')\n",
    "    print(f'X_test error: {RMSE(P, Q, X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "555813f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dfs(P, Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
